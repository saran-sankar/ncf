{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Collaborative Filtering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python3 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanstykes/ncf/blob/master/Neural_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SKi7EklK3mIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative Filtering\n",
        "\n",
        "Neural network based collaborative filtering for recommending new products by analyzing feedbacks from users. Intended to be utilized in areas including movies, music, news, books, and products in general. In this project, I demonstrate movie recommandation using the Netflix Prize dataset, learning from implcit feedbacks."
      ]
    },
    {
      "metadata": {
        "id": "le_vESdI3mIx",
        "colab_type": "code",
        "colab": {},
        "outputId": "2628d712-a264-46ba-c90b-ec491309170b"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/me/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-a2gwtj33mI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extracting data from files to create a user-movie matrix\n",
        "The datasets contain over 100 million ratings from 480 thousand\n",
        "randomly-chosen, anonymous Netflix customers over 17 thousand movie titles.\n",
        "\n",
        "The \"training_set\" directory contains 17770 files, one\n",
        "per movie.  The first line of each file contains the movie id followed by a\n",
        "colon.  Each subsequent line in the file corresponds to a rating from a customer\n",
        "and its date in the following format:\n",
        "\n",
        "CustomerID,Rating,Date\n",
        "\n",
        "- MovieIDs range from 1 to 17770 sequentially.\n",
        "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
        "- Ratings are on a five star (integral) scale from 1 to 5.\n",
        "- Dates have the format YYYY-MM-DD.\n",
        "\n",
        "We ignore the dates and extract user id's and corresponding movie ratings to form a user-movie matrix."
      ]
    },
    {
      "metadata": {
        "id": "Y0w1_2JC3mI1",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d64e389-4443-4c96-b43e-3e63550bc56a"
      },
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "num_movies = 17770\n",
        "num_user_ids = 2649429 \n",
        "num_users = 480189\n",
        "user_movies = np.zeros((num_users, num_movies))\n",
        "user_dict = {} # user_id -> user_row\n",
        "movie_ids = []\n",
        "\n",
        "user_count = 0\n",
        "file_count = 0\n",
        "\n",
        "for filename in os.listdir(\"dataset/training_set/\"):\n",
        "    movie_file = open(\"dataset/training_set/\"+filename)\n",
        "    movie_data = movie_file.read().split(\"\\n\")\n",
        "    movie_id = int(movie_data[0].strip(\":\"))\n",
        "    movie_ids.append(movie_id)\n",
        "    #print(movie_id)\n",
        "    for i in range(1, len(movie_data) -1 ):\n",
        "        user_rating = movie_data[i].split(\",\")\n",
        "        user_id = user_rating[0]\n",
        "        rating = user_rating[1]\n",
        "        #print(user_id)\n",
        "        if user_id not in user_dict:\n",
        "            user_dict[user_id] = user_count\n",
        "            user_movies[user_count, movie_id - 1] = rating\n",
        "            user_count += 1\n",
        "        else:\n",
        "            user_movies[user_dict[user_id], movie_id - 1] = rating\n",
        "    if movie_id > 17770: \n",
        "        break\n",
        "    if file_count%1000 == 0:\n",
        "        print(\"Files loaded:\", file_count)\n",
        "    file_count+=1\n",
        "    \n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))\n",
        "print(\"number of users:\", user_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files loaded: 0\n",
            "Files loaded: 1000\n",
            "Files loaded: 2000\n",
            "Files loaded: 3000\n",
            "Files loaded: 4000\n",
            "Files loaded: 5000\n",
            "Files loaded: 6000\n",
            "Files loaded: 7000\n",
            "Files loaded: 8000\n",
            "Files loaded: 9000\n",
            "Files loaded: 10000\n",
            "Files loaded: 11000\n",
            "Files loaded: 12000\n",
            "Files loaded: 13000\n",
            "Files loaded: 14000\n",
            "Files loaded: 15000\n",
            "Files loaded: 16000\n",
            "Files loaded: 17000\n",
            "time elapsed: 666.4024882316589\n",
            "number of users: 480189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nVzWvP_v3mI5",
        "colab_type": "code",
        "colab": {},
        "outputId": "b95a1c01-37a6-4cb1-a8b3-9ba79cf9be3f"
      },
      "cell_type": "code",
      "source": [
        "#analyze the data\n",
        "user_id = 1488844\n",
        "movie_id = 1\n",
        "\n",
        "print(user_movies[user_dict[str(user_id)], movie_id - 1])\n",
        "j=0\n",
        "for rating in user_movies[user_dict[\"1956732\"]]:\n",
        "    if rating>0:\n",
        "        j+=1\n",
        "print(j)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3RPijBMk3mI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MebfCei_3mI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load movie titles\n",
        "titles_file = open(\"dataset/movie_titles.txt\", encoding = \"ISO-8859-1\")\n",
        "movie_info = titles_file.read().split(\"\\n\")\n",
        "movie_ids_titles = {}\n",
        "for i in range(len(movie_info)):\n",
        "    info_split = movie_info[i].split(\",\")\n",
        "    #print(info_split[2])\n",
        "    movie_ids_titles[str(info_split[0])] = info_split[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ic0qAeWA3mJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81QcPjYB3mJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optional* Step: Processing the user-movie matrix to create an input matrix with sparse vectors as rows\n",
        "\n",
        "Each row of the input matrix will contain a concatenation of feature vectors of users and movies. Corresponding ratings are stored in a different vector.\n",
        "#### *Optional for stochastic gradient descent and not required for minibatch gradient descent"
      ]
    },
    {
      "metadata": {
        "id": "rsRNhU2E3mJG",
        "colab_type": "code",
        "colab": {},
        "outputId": "30035747-d228-4a16-83ea-d4f356ece330"
      },
      "cell_type": "code",
      "source": [
        "tic = time.time()\n",
        "\n",
        "user_movies_train_users = user_movies[:10000]\n",
        "#print(user_movies_train_users.shape)\n",
        "#user_movies_test = user_movies[336132:]\n",
        "\n",
        "nonzero_indices = np.nonzero(user_movies_train_users)\n",
        "nonzero_indices = np.array([nonzero_indices[0], nonzero_indices[1]]) #do the shuffle after this\n",
        "\n",
        "#shuffle\n",
        "np.random.shuffle(nonzero_indices.T)\n",
        "count_nonzero_indices = 100\n",
        "nonzero_indices = nonzero_indices[:, :count_nonzero_indices]\n",
        "\n",
        "users = nonzero_indices[0]\n",
        "movies = nonzero_indices[1]\n",
        "\n",
        "print(\"Number of ratings:\", count_nonzero_indices)\n",
        "user_movies_train = np.zeros((count_nonzero_indices, num_users + num_movies))\n",
        "#print(user_movies_train.shape)\n",
        "ratings = np.zeros((count_nonzero_indices))\n",
        "#print(ratings.shape)\n",
        "#user_movies_log = np.empty((count_nonzero_indices, 2))\n",
        "#user_movies_train[0] = np.ones((num_users + num_movies, 1))\n",
        "\n",
        "for i in range(count_nonzero_indices):\n",
        "    rating = user_movies_train_users[users[i], movies[i]]\n",
        "    ratings[i] = rating\n",
        "    user_vector = np.expand_dims(user_movies[users[i]], axis =1)\n",
        "    #print(user_vector.shape)\n",
        "    movie_vector = np.expand_dims(user_movies[:, movies[i]] , axis=1)\n",
        "    #print(movie_vector.shape)\n",
        "    user_movies_train[i] = np.concatenate((user_vector, movie_vector), axis=0)[:,0]\n",
        "    #user_movies_log[i][0], user_movies_log[i][1] = (users[i], movies[i])\n",
        "    if(i%100 == 0):\n",
        "        print(\"completed:\", i)\n",
        "    \n",
        "print(ratings)\n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of ratings: 100\n",
            "completed: 0\n",
            "[3. 4. 5. 4. 3. 1. 4. 3. 4. 4. 2. 4. 2. 3. 3. 5. 4. 4. 5. 1. 4. 5. 3. 3.\n",
            " 3. 4. 2. 4. 5. 1. 5. 4. 3. 4. 3. 1. 5. 4. 3. 4. 4. 2. 4. 3. 5. 4. 1. 3.\n",
            " 2. 3. 3. 3. 4. 3. 5. 3. 5. 1. 4. 4. 4. 5. 3. 5. 4. 5. 1. 3. 2. 3. 5. 2.\n",
            " 4. 5. 3. 4. 4. 3. 3. 3. 3. 3. 2. 3. 5. 4. 4. 4. 5. 3. 5. 4. 3. 3. 5. 2.\n",
            " 4. 5. 4. 3.]\n",
            "time elapsed: 230.9734058380127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JKwomfK43mJN",
        "colab_type": "code",
        "colab": {},
        "outputId": "1281f48b-fbde-46ed-f3b7-ce2e8bf615b0"
      },
      "cell_type": "code",
      "source": [
        "#print(np.count_nonzero(user_movies_train[:,0]))\n",
        "#print(user_movies_log[1])\n",
        "print(nonzero_indices)\n",
        "#qqqqq = nonzero_indices[0][0]\n",
        "#print(user_dict[str(qqqqq)]) #do the reverse instead"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6014  2558  5395  8807   616  7258  2801  3978  1027  4851  6730  2156\n",
            "   8613   130  6375  9606  7576  8064  1918  2040  3104  7255   182  3283\n",
            "   9822  6411  2177  5413  1985  8703  8936  1324  5189  6930  3569  2024\n",
            "    475  9201   213  5148  4954  4964  5675   543   336   631  3322    96\n",
            "   2466  2301  7895  1383  8360  1149  1928  2764  6407    73   650  3293\n",
            "    861  2559  6688   340  1802  8692  1552  8897  8870  3322  6540  9029\n",
            "   7618  4753  4453   962  4762   164  9450  2119  8962  2071  3737  3449\n",
            "   3492  1301  2421  8976  9071  3329  6261  2592  2421  2416  2573    92\n",
            "   1328  7473   183  1581]\n",
            " [ 1831 11278  4632  7816 14730  8595  4989 12842 16533 12434 11638  4298\n",
            "  14617 14046 17096 13808 12014 15199 13613 11254  8727  8467    45 13794\n",
            "   3112  4419 10893 14049  7033  7009 14868 10995 10175  6427  5205 12416\n",
            "   5774  7232  5632 13301  8650  6608 14311  6971 12842 15208  5312  8049\n",
            "   9420  5938  7612  5961 15840 12458  3961   482  7232 12209  6165  4224\n",
            "   3637   453  8338 13391  4846 11083 16900  5316  6267  3419  4885  4392\n",
            "    142 11902  5588 14390 13391   732  1531 10549 15163  3941  6734  2187\n",
            "   5447  6481  3488 10892  6696 10834 14583 17557 14778 11887  5938  5792\n",
            "   8903   330 10108  6959]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwEX3CbP3mJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFmtfdnm3mJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ]
    },
    {
      "metadata": {
        "id": "kVi0sTLj3mJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y, m):\n",
        "    \n",
        "    x = tf.placeholder(tf.float32, [n_x,m])#497959\n",
        "    y = tf.placeholder(tf.float32, [n_y,m])\n",
        "    \n",
        "    return x,y\n",
        "\n",
        "def initialize_parameters(n_x, n_y):\n",
        "    \n",
        "    W1 = tf.get_variable(\"W1\", [25, n_x], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [n_y, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b3 = tf.get_variable(\"b3\", [n_y,1], initializer = tf.zeros_initializer())\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def forward_propagation(x, parameters): \n",
        "    \n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    \n",
        "    Z1 = tf.add(tf.matmul(W1, x), b1)\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
        "    y_hat = Z3 #tf.minimum(5.0, tf.maximum(0.0, Z3))\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWZWldkK3mJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "EaPOtKVq3mJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent_model(num_epochs, training_sample_size, use_train_matrix):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x, y = create_placeholders(num_users + num_movies, 1, 1) #497959\n",
        "\n",
        "    parameters = initialize_parameters(num_users + num_movies, 1) #497959\n",
        "\n",
        "    y_hat = forward_propagation(x, parameters)\n",
        "\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y_hat), labels=tf.transpose(y)))\n",
        "    #print(y_hat.shape, y.shape)\n",
        "    cost = tf.losses.mean_squared_error(y, y_hat)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
        "        beta1=0.9,\n",
        "        beta2=0.999,\n",
        "        epsilon=1e-08,\n",
        "        use_locking=False,\n",
        "        name='Adam').minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            print(\"epoch\",epoch+1)\n",
        "            epoch_cost = 0\n",
        "            divisor = 0\n",
        "            \n",
        "            if use_train_matrix == False:\n",
        "                for user_row in range(0,training_sample_size):\n",
        "                    #print(\"user\", user_row + 1)\n",
        "                    for movie_id, rating in enumerate(user_movies[user_row]):\n",
        "                        X=[]\n",
        "                        if rating>0:\n",
        "                            user_matrix = user_movies[user_row]\n",
        "                            movie_matrix = user_movies[:, movie_id]\n",
        "                            X = np.expand_dims(np.concatenate((user_matrix, movie_matrix)), axis=1)\n",
        "                            #print(X.shape)\n",
        "                            Y = np.expand_dims(np.expand_dims(rating, axis=1),axis=1)\n",
        "                            #print(Y)\n",
        "                            _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                            epoch_cost += cost_\n",
        "                            divisor += 1\n",
        "                            break\n",
        "                            \n",
        "            elif use_train_matrix == True:\n",
        "                for index, sparse_vector in enumerate(user_movies_train):\n",
        "                    X = np.expand_dims(sparse_vector, axis=1)\n",
        "                    Y = np.expand_dims(np.expand_dims(ratings[index], axis=1), axis=1)\n",
        "                    _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                    epoch_cost += cost_\n",
        "                    divisor += 1\n",
        "                    if (index>training_sample_size):\n",
        "                        break\n",
        "                        #pass\n",
        "            \n",
        "            epoch_cost /= divisor\n",
        "            print(\"training loss:\", epoch_cost,\"\\n\")\n",
        "            \n",
        "        parameters = sess.run(parameters)\n",
        "        return parameters\n",
        "    \n",
        "def test_stochastic_gradient_descent_model(parameters, test_sample_size, show_predictions, use_train_matrix):\n",
        "    \n",
        "    cost = 0\n",
        "    test_sample_users = np.random.randint(count_nonzero_indices - training_sample_size, size=(test_sample_size,1)) + training_sample_size\n",
        "    #print(test_sample_users)\n",
        "    \n",
        "    for i in range(test_sample_size):\n",
        "        \n",
        "        j = test_sample_users[i][0]\n",
        "        \n",
        "        if use_train_matrix == False:\n",
        "            \n",
        "            user_vector = np.expand_dims(user_movies[users[j]], axis =1)\n",
        "            #print(user_vector.shape)\n",
        "            movie_vector = np.expand_dims(user_movies[:, movies[j]] , axis=1)\n",
        "            #print(movie_vector.shape)\n",
        "            X_predict = np.concatenate((user_vector, movie_vector), axis=0)\n",
        "            X_predict = tf.cast(X_predict, tf.float32)\n",
        "            \n",
        "        elif use_train_matrix == True:\n",
        "            \n",
        "            X_predict = np.expand_dims(user_movies_train[j], axis=1)\n",
        "            X_predict = tf.cast(X_predict, tf.float32)\n",
        "            \n",
        "        prediction = forward_propagation(X_predict, parameters)\n",
        "        #actual_rating = user_movies_train_users[users[j], movies[j]]\n",
        "        actual_rating = ratings[j]\n",
        "        #actual_rating = expand_dims(actual_rating, axis=1)\n",
        "        \n",
        "        if show_predictions == 1:\n",
        "            sess = tf.Session()\n",
        "            print(\"prediction:\", min(max(round(sess.run(prediction)[0][0]),0.0),5.0))\n",
        "            sess.close()\n",
        "            print(\"actual rating:\", actual_rating,\"\\n\")\n",
        "        \n",
        "        cost += tf.losses.mean_squared_error(actual_rating, prediction[0,0])\n",
        "        \n",
        "    cost /= test_sample_size\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfCsw9-S3mJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recommend_movies_1(user_id, parameters, search_space_size, number_of_recommendations):\n",
        "    \n",
        "        predictions = np.zeros(search_space_size)\n",
        "        search_space_movies = np.random.randint(movies.shape[0], size=(movies.shape[0], 1))\n",
        "        \n",
        "        #print(search_space_movies)\n",
        "        #print(movies[search_space_movies[0][0]])\n",
        "        \n",
        "        for i in range(search_space_size):\n",
        "        \n",
        "            j = search_space_movies[i][0]\n",
        "            \n",
        "            tic = time.time()\n",
        "            user_vector = np.expand_dims(user_movies[user_dict[user_id]], axis =1)\n",
        "            movie_vector = np.expand_dims(user_movies[:, movies[j]] , axis = 1)\n",
        "            X_predict = np.concatenate((user_vector, movie_vector), axis=0)\n",
        "            X_predict = tf.cast(X_predict, tf.float32)\n",
        "            #print(\"Time elapsed for predicting:\",time.time() - tic)\n",
        "            \n",
        "            tic = time.time()\n",
        "            prediction = tf.Session().run(forward_propagation(X_predict, parameters))\n",
        "            #print(\"Time elapsed for calculating:\",time.time() - tic)\n",
        "            \n",
        "            predictions[i] = prediction\n",
        "            \n",
        "        i_max = np.argmax(predictions) \n",
        "        j_max = search_space_movies[i][0]    \n",
        "        return movies[j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sZ-e_hxv3mJa",
        "colab_type": "code",
        "colab": {},
        "outputId": "5cb561b4-3ad2-4553-900d-5df4c5cee082"
      },
      "cell_type": "code",
      "source": [
        "#train\n",
        "training_sample_size = 90\n",
        "tic = time.time()\n",
        "parameters = stochastic_gradient_descent_model(num_epochs = 20, training_sample_size = training_sample_size, use_train_matrix = True)\n",
        "toc = time.time()\n",
        "print(\"time elapsed:\",(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/me/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 2039.1399489683706 \n",
            "\n",
            "epoch 2\n",
            "training loss: 3352.207876700746 \n",
            "\n",
            "epoch 3\n",
            "training loss: 3652.7607915136305 \n",
            "\n",
            "epoch 4\n",
            "training loss: 200.2159911255604 \n",
            "\n",
            "epoch 5\n",
            "training loss: 64.04948830073599 \n",
            "\n",
            "epoch 6\n",
            "training loss: 37.56620274251066 \n",
            "\n",
            "epoch 7\n",
            "training loss: 35.260375770598486 \n",
            "\n",
            "epoch 8\n",
            "training loss: 31.865332347461816 \n",
            "\n",
            "epoch 9\n",
            "training loss: 35.967080878103964 \n",
            "\n",
            "epoch 10\n",
            "training loss: 47.07712229557776 \n",
            "\n",
            "epoch 11\n",
            "training loss: 21.960863165056292 \n",
            "\n",
            "epoch 12\n",
            "training loss: 26.492141803311508 \n",
            "\n",
            "epoch 13\n",
            "training loss: 17.564287356991805 \n",
            "\n",
            "epoch 14\n",
            "training loss: 13.240431679787276 \n",
            "\n",
            "epoch 15\n",
            "training loss: 20.33603730859961 \n",
            "\n",
            "epoch 16\n",
            "training loss: 18.19219016924541 \n",
            "\n",
            "epoch 17\n",
            "training loss: 13.682747877825813 \n",
            "\n",
            "epoch 18\n",
            "training loss: 10.981376668265591 \n",
            "\n",
            "epoch 19\n",
            "training loss: 10.256042455854502 \n",
            "\n",
            "epoch 20\n",
            "training loss: 13.898232973692085 \n",
            "\n",
            "time elapsed: 87.01115703582764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UsInDlx23mJc",
        "colab_type": "code",
        "colab": {},
        "outputId": "8ca2b2dd-9f7d-4051-e4b4-efcbbf9269d8"
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "cost = test_stochastic_gradient_descent_model(parameters, test_sample_size = 10, show_predictions = True, use_train_matrix = True)\n",
        "sess = tf.Session()\n",
        "print(\"test loss:\", sess.run(cost))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: 1.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "prediction: 3.0\n",
            "actual rating: 4.0 \n",
            "\n",
            "prediction: 4.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 4.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "prediction: 1.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "prediction: 2.0\n",
            "actual rating: 2.0 \n",
            "\n",
            "prediction: 1.0\n",
            "actual rating: 3.0 \n",
            "\n",
            "prediction: 4.0\n",
            "actual rating: 5.0 \n",
            "\n",
            "test loss: 2.7258148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aZ_w7pUy3mJe",
        "colab_type": "code",
        "colab": {},
        "outputId": "d88b8179-9dee-43f1-e967-1faa8a12ca6a"
      },
      "cell_type": "code",
      "source": [
        "#recommend movie\n",
        "#tic = time.time()\n",
        "user_s_id = \"1488844\"\n",
        "recommended_movie_id = recommend_movies_1(user_s_id, parameters, search_space_size = 10, number_of_recommendations = 1)\n",
        "print(\"Recommended movie: \",movie_ids_titles[str(recommended_movie_id)])\n",
        "\n",
        "#print(\"Time elapsed:\",time.time() - tic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recommended movie:  Babe: Pig in the City\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvu-ELQY3mJh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4JnB3AgT3mJk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DON'T RUN\n",
        "#predict\n",
        "user_id = 1025579\n",
        "movie_id = 1\n",
        "\n",
        "X_predict = np.expand_dims(np.concatenate((user_movies[user_dict[str(user_id)]], user_movies[:, movie_id - 1])), axis=1)\n",
        "X_predict = tf.cast(X_predict, tf.float32)\n",
        "prediction = forward_propagation(X_predict, parameters)\n",
        "\n",
        "sess = tf.Session()\n",
        "predicted_rating = sess.run(prediction)[0,0]\n",
        "actual_rating = user_movies[user_dict[str(user_id)], movie_id - 1]\n",
        "print(\"predicted rating:\", predicted_rating)\n",
        "print(\"actual rating:\", actual_rating)\n",
        "print(\"cost:\", sess.run(tf.losses.mean_squared_error(actual_rating, predicted_rating)))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNNl2TPX3mJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Minibatch Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "rqp7p6hl3mJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def minibatch_gradient_descent_model(num_epochs, training_sample_size, minibatch_size, show_ratings):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x, y = create_placeholders(num_users + num_movies, 1, minibatch_size)\n",
        "\n",
        "    parameters = initialize_parameters(num_users + num_movies, 1)\n",
        "\n",
        "    y_hat = forward_propagation(x, parameters)\n",
        "\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y_hat), labels=tf.transpose(y)))\n",
        "    #print(y_hat.shape, y.shape)\n",
        "    cost = tf.losses.mean_squared_error(y, y_hat)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
        "        beta1=0.9,\n",
        "        beta2=0.999,\n",
        "        epsilon=1e-08,\n",
        "        use_locking=False,\n",
        "        name='Adam').minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "        \n",
        "        movie_id = np.random.randint(0,user_movies.shape[1] - minibatch_size)\n",
        "        for epoch in range(num_epochs):\n",
        "            print(\"\\nepoch\",epoch+1)\n",
        "            epoch_cost = 0\n",
        "            num_of_samples = 0\n",
        "            user_id_start = np.random.randint(0, 100)\n",
        "            for user_row in range(user_id_start, user_id_start + training_sample_size, minibatch_size):\n",
        "                #print(\"user\", user_row + 1)\n",
        "                movie_matrix = np.transpose(user_movies[:, movie_id:movie_id + minibatch_size])\n",
        "                user_matrix = user_movies[user_row,:]\n",
        "                user_matrix = np.multiply(user_matrix, np.ones((minibatch_size, user_matrix.shape[0])))\n",
        "                #print(movie_matrix.shape)\n",
        "                #print(user_matrix.shape, movie_matrix.shape)\n",
        "                X = np.concatenate((user_matrix, movie_matrix), axis=1).T\n",
        "                #print(X.shape)\n",
        "                #Y = np.expand_dims(np.expand_dims(rating, axis=1),axis=1) #do the diagonal\n",
        "                Y = np.expand_dims(user_movies[user_row, movie_id:movie_id + minibatch_size], axis=0)\n",
        "                if (show_ratings == True):\n",
        "                    print(Y)\n",
        "                _ , cost_ = sess.run([optimizer,cost], feed_dict={x:X,y:Y})\n",
        "                epoch_cost += cost_\n",
        "                num_of_samples += 1\n",
        "            print(\"training loss:\", epoch_cost/num_of_samples)\n",
        "        parameters = sess.run(parameters)\n",
        "        return parameters\n",
        "\n",
        "def test_minibatch_gradient_descent_model(parameters, test_sample_size, minibatch_size, show_predictions):\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "        cost = 0\n",
        "        num_of_samples = 0\n",
        "        \n",
        "        movie_id = np.random.randint(0,user_movies.shape[1] - minibatch_size)\n",
        "        user_id_start = np.random.randint(100, 1000)\n",
        "        \n",
        "        for user_row in range(user_id_start, user_id_start + test_sample_size, minibatch_size):\n",
        "            \n",
        "            num_of_samples += 1\n",
        "            movie_matrix = np.transpose(user_movies[:, movie_id:movie_id + minibatch_size])\n",
        "            user_matrix = user_movies[user_row,:]\n",
        "            user_matrix = np.multiply(user_matrix, np.ones((minibatch_size, user_matrix.shape[0])))\n",
        "\n",
        "            X = np.concatenate((user_matrix, movie_matrix), axis=1).T\n",
        "            X = tf.cast(X, tf.float32)\n",
        "\n",
        "            Y = np.expand_dims(user_movies[user_row, movie_id:movie_id + minibatch_size], axis=0)\n",
        "            \n",
        "            prediction = tf.Session().run(forward_propagation(X, parameters))\n",
        "            \n",
        "            if (show_predictions == True):\n",
        "                print(\"predictions:\", -np.round(prediction)[0], \"\\nactual ratings:\", Y[0])\n",
        "                print(\"argmax predicted:\", (-prediction)[0].argsort()[-3:], \"\\nactual ratings:\", Y[0].argsort()[-3:])\n",
        "                \n",
        "            cost += tf.losses.mean_squared_error(Y, prediction)\n",
        "            \n",
        "    cost /= num_of_samples\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8zDuIel3mJp",
        "colab_type": "code",
        "colab": {},
        "outputId": "b153c2c4-e87c-449f-9c06-b1c5c0bc5493"
      },
      "cell_type": "code",
      "source": [
        "#train\n",
        "tic = time.time()\n",
        "minibatch_size = 100\n",
        "training_sample_size = 500\n",
        "num_epochs = 5\n",
        "parameters = minibatch_gradient_descent_model(num_epochs, training_sample_size, minibatch_size, show_ratings = False)\n",
        "toc = time.time()\n",
        "print(\"\\ntime elapsed:\",(toc - tic))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "training loss: 1428.8088837563992\n",
            "\n",
            "epoch 2\n",
            "training loss: 60.826647186279295\n",
            "\n",
            "epoch 3\n",
            "training loss: 25.592811584472656\n",
            "\n",
            "epoch 4\n",
            "training loss: 10.708226680755615\n",
            "\n",
            "epoch 5\n",
            "training loss: 7.147312760353088\n",
            "\n",
            "time elapsed: 91.45508909225464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bcv3zae_3mJr",
        "colab_type": "code",
        "colab": {},
        "outputId": "5e75fa68-f9db-400c-fd05-0eaf838dcf3f"
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "test_sample_size = 200\n",
        "test_loss = test_minibatch_gradient_descent_model(parameters, test_sample_size, minibatch_size, show_predictions = False)\n",
        "sess = tf.Session()\n",
        "print(\"test loss:\", sess.run(test_loss))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: 20.681131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxPaEQmz3mJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recommend_movies_2(user_id, parameters, minibatch_size):\n",
        "    \n",
        "    user_row = user_dict[user_id]\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(init)\n",
        "    \n",
        "        movie_id = np.random.randint(0,user_movies.shape[1] - minibatch_size)\n",
        "        movie_matrix = np.transpose(user_movies[:, movie_id:movie_id + minibatch_size])\n",
        "        user_matrix = user_movies[user_row,:]\n",
        "        user_matrix = np.multiply(user_matrix, np.ones((minibatch_size, user_matrix.shape[0])))\n",
        "\n",
        "        X = np.concatenate((user_matrix, movie_matrix), axis=1).T\n",
        "        X = tf.cast(X, tf.float32)\n",
        "        \n",
        "        prediction = tf.Session().run(forward_propagation(X, parameters))\n",
        "\n",
        "        return prediction, movie_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmES_z-h3mJv",
        "colab_type": "code",
        "colab": {},
        "outputId": "14900c9f-96e6-4136-92ee-b4f9d5456e9c"
      },
      "cell_type": "code",
      "source": [
        "#recommend movie\n",
        "number_of_recommendations = 3\n",
        "user_s_id = \"1488844\"\n",
        "prediction, movie_id_start = recommend_movies_2(user_id = user_s_id, parameters = parameters, minibatch_size = 50)\n",
        "recommended_movie_id = movie_id_start + (-prediction)[0].argsort()[-number_of_recommendations:]\n",
        "print(-np.round(prediction)[0])\n",
        "for i in range(number_of_recommendations-1, -1,-1):\n",
        "    print(\"Recommended movie: \", movie_ids_titles[str(recommended_movie_id[i])])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.  1.  0. 12.  1.  0.  1.  1.  0.  1.  1.  2.  1.  1.  0.  1.  1.  1.\n",
            "  1. 10.  1.  0.  0.  1. 10.  1.  2.  1.  1.  0.  1.  1.  2.  0.  0.  1.\n",
            "  1.  1.  2.  1.  1.  0.  4. -2.  1.  1.  1.  1.  1.  1.]\n",
            "Recommended movie:  Satan's Brew\n",
            "Recommended movie:  La Belle Noiseuse\n",
            "Recommended movie:  The Trench\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYVRB8Tg3mJy",
        "colab_type": "code",
        "colab": {},
        "outputId": "7d7ef5a5-916f-435d-8074-41d8c538d97f"
      },
      "cell_type": "code",
      "source": [
        "DON'T RUN\n",
        "user_id = 321111\n",
        "movie_id = 2\n",
        "\n",
        "X_test = np.expand_dims(np.concatenate((user_movies[user_dict[str(user_id)]], user_movies[:, movie_id - 1])), axis=1)\n",
        "X_test = tf.cast(X_test, tf.float32)\n",
        "print(X_test)\n",
        "prediction = forward_propagation(X_test, parameters)\n",
        "sess = tf.Session()\n",
        "print(np.math.floor(sess.run(prediction)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Cast:0\", shape=(497959, 1), dtype=float32)\n",
            "-1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X75kunDq3mJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}